{"cells":[{"cell_type":"markdown","id":"758a73ce-4c69-4871-83ab-a9ff8f48f229","metadata":{},"outputs":[],"source":["\n","<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n","    </a>\n","</p>\n","\n","\n","# K-means Clustering\n","\n","\n","Estimated time needed: **25** minutes\n","    \n","\n","## Objectives\n","\n","After completing this lab you will be able to:\n","\n","* Use scikit-learn's k-means clustering to cluster data\n","* Apply k-means clustering on a real world data for Customer segmentation\n"]},{"cell_type":"markdown","id":"3f97b3a1-7fdc-4952-b745-07df92cd5cba","metadata":{},"outputs":[],"source":["## Introduction\n","\n","**K-means** is vastly used for clustering in many data science applications. It is especially useful if you need to quickly discover insights from **unlabeled data**.\n","\n","Real-world applications of k-means include:\n","- Customer segmentation\n","- Understanding what website visitors are trying to accomplish\n","- Pattern recognition\n","- Feature engineering \n","- Data compression\n"]},{"cell_type":"markdown","id":"42edc3d3-db48-415e-9169-06abae6b9989","metadata":{},"outputs":[],"source":["### Install libraries\n","Ensure the required libraries are available.\n"]},{"cell_type":"code","id":"7a03fd97-88fe-4e22-b867-96dc719466bc","metadata":{},"outputs":[],"source":["!pip install numpy\n!pip install pandas\n!pip install matplotlib\n!pip install scikit-learn\n!pip install plotly\n!pip install seaborn"]},{"cell_type":"markdown","id":"9faaefb5-d3b5-4f30-9b51-7689f51bb170","metadata":{},"outputs":[],"source":["Now import the required libraries.\n"]},{"cell_type":"code","id":"a2014509-89f1-4cc4-bcbe-a99da7238e9a","metadata":{},"outputs":[],"source":["import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt \nfrom sklearn.cluster import KMeans \nfrom sklearn.datasets import make_blobs \nfrom sklearn.preprocessing import StandardScaler\nimport plotly.express as px\nimport seaborn as sns\n\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')"]},{"cell_type":"markdown","id":"d044db7b-d081-442a-b8f8-13056257e0b3","metadata":{},"outputs":[],"source":["<h1 id=\"random_generated_dataset\">K-Means on a synthetic data set</h1>\n","\n","Let's create our own dataset for this lab!\n"]},{"cell_type":"markdown","id":"efd897cb-be15-420b-b200-e1680d1c5d4b","metadata":{},"outputs":[],"source":["First, we need to set a random seed. Use <b>numpy's random.seed()</b> function, where the seed will be set to <b>0</b>.\n"]},{"cell_type":"code","id":"3ad308e9-28bb-4da0-875f-324df13cbbf9","metadata":{},"outputs":[],"source":["np.random.seed(0)"]},{"cell_type":"markdown","id":"a2fa81b3-fdd4-47a2-b6dd-4e8aa7d5d394","metadata":{},"outputs":[],"source":["Next, we will be making random clusters of points by using the `make_blobs` class. The `make_blobs` class can take in many inputs, but we will be using these specific ones. <br> <br>\n","<b> <u> Input </u> </b>\n","<ul>\n","    <li> <b>n_samples</b>: The total number of points equally divided among clusters. </li>\n","    <ul> <li> Value will be: 5000 </li> </ul>\n","    <li> <b> centres </b>: The number of centres to generate, or the fixed centre locations. </li>\n","    <ul> <li> Value will be: [[4, 4], [-2, -1], [2, -3],[1,1]] </li> </ul>\n","    <li> <b>cluster_std</b>: The standard deviation of the clusters. </li>\n","    <ul> <li> Value will be: 0.9 </li> </ul>\n","</ul>\n","<br>\n","<b> <u> Output </u> </b>\n","<ul>\n","    <li> <b>X</b>: Array of shape [n_samples, n_features]. (Feature Matrix)</li>\n","    <ul> <li> The generated samples. </li> </ul> \n","    <li> <b>y</b>: Array of shape [n_samples]. (Response Vector)</li>\n","    <ul> <li> The integer labels for cluster membership of each sample. </li> </ul>\n","</ul>\n"]},{"cell_type":"code","id":"b610c2f2-4628-4103-8361-f9920a4b3124","metadata":{},"outputs":[],"source":["X, y = make_blobs(n_samples=5000, centers=[[4,4], [-2, -1], [2, -3], [1, 1]], cluster_std=0.9)"]},{"cell_type":"markdown","id":"c69bfa84-2d6b-4ad2-9052-58d13214ffdf","metadata":{},"outputs":[],"source":["Display the scatter plot of the randomly generated data.\n"]},{"cell_type":"code","id":"5531f2f1-3f34-4d5a-98de-aa86f0ad9a60","metadata":{},"outputs":[],"source":["plt.scatter(X[:, 0], X[:, 1], marker='.',alpha=0.3,ec='k',s=80)"]},{"cell_type":"markdown","id":"ce82a725-c384-41cb-bca6-199c37149af7","metadata":{},"outputs":[],"source":["<h2 id=\"setting_up_K_means\">Setting up k-means</h2>\n","Now that we have our random data, let's set up our k-means Clustering.\n"]},{"cell_type":"markdown","id":"232eb2dd-f8a9-4baa-a6e5-f8da50f8581b","metadata":{},"outputs":[],"source":["The KMeans class has many parameters that can be used, but we will be using these three:\n","\n","- `init`: Initialization method of the centroids.\n","  - Value will be: `k-means++` \n","  - `k-means++`: Selects initial cluster centres for k-means clustering in a smart way to speed up convergence.\n","- `n_clusters`: The number of clusters to form as well as the number of centroids to generate. \n","  -  Value will be: 4 (since we have 4 centres) \n","- `n_init`: Number of times the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia. \n","   - Value will be: 12  \n","\n","\n","Initialize `KMeans` with these parameters, where the output variable is called `k_means`.\n"]},{"cell_type":"code","id":"4ea8579e-93eb-47ff-b601-3c62df4445cc","metadata":{},"outputs":[],"source":["k_means = KMeans(init = \"k-means++\", n_clusters = 4, n_init = 12)"]},{"cell_type":"markdown","id":"cd8b195a-a701-4832-abf2-3bd6b7c55f2e","metadata":{},"outputs":[],"source":["Now let's fit the `KMeans` model with the feature matrix we created above, <b> X </b>.\n"]},{"cell_type":"code","id":"925fec06-3149-4cf8-8040-8c05671399ae","metadata":{},"outputs":[],"source":["k_means.fit(X)"]},{"cell_type":"markdown","id":"6af8135a-42b3-4eae-a27d-48634339ecf0","metadata":{},"outputs":[],"source":["Now let's get the label for each point in the model using the `k_means.labels_` attribute and save them as `k_means_labels`.\n"]},{"cell_type":"code","id":"d25c2877-41a7-454b-a92e-7ed3dfd02115","metadata":{},"outputs":[],"source":["k_means_labels = k_means.labels_\nk_means_labels"]},{"cell_type":"markdown","id":"9ab1dfe6-5841-47cd-998a-a9ccab27b3aa","metadata":{},"outputs":[],"source":["We will also get the coordinates of the cluster centers using `k_means.cluster_centers_` and save it as `k_means_cluster_centers`.\n"]},{"cell_type":"code","id":"0e0c5a97-b2e9-4c4a-9893-b9d96b685655","metadata":{},"outputs":[],"source":["k_means_cluster_centers = k_means.cluster_centers_\nk_means_cluster_centers"]},{"cell_type":"markdown","id":"b1352e4c-0f97-4bff-83a0-148c6d3be46a","metadata":{},"outputs":[],"source":["<h2 id=\"creating_visual_plot\">Creating the Visual Plot</h2>\n","\n","Now that we have the random data generated and the k-means model initialized, let's plot the result and see what it looks like!\n"]},{"cell_type":"markdown","id":"25703771-322d-4e60-ae5f-2cd3bd06e801","metadata":{},"outputs":[],"source":["Please read through the code and comments to understand how to plot the model.\n"]},{"cell_type":"code","id":"17edacfe-f468-4122-86ab-df59ef1f1f7d","metadata":{},"outputs":[],"source":["# Initialize the plot with the specified dimensions.\nfig = plt.figure(figsize=(6, 4))\n\n# Colors uses a color map, which will produce an array of colors based on\n# the number of labels there are. We use set(k_means_labels) to get the\n# unique labels.\ncolors = plt.cm.tab10(np.linspace(0, 1, len(set(k_means_labels))))\n\n# Create a plot\nax = fig.add_subplot(1, 1, 1)\n\n# For loop that plots the data points and centroids.\n# k will range from 0-3, which will match the possible clusters that each\n# data point is in.\nfor k, col in zip(range(len([[4, 4], [-2, -1], [2, -3], [1, 1]])), colors):\n\n    # Create a list of all data points, where the data points that are \n    # in the cluster (ex. cluster 0) are labeled as true, else they are\n    # labeled as false.\n    my_members = (k_means_labels == k)\n\n    # Define the centroid, or cluster center.\n    cluster_center = k_means_cluster_centers[k]\n\n    # Plots the datapoints with color col.\n    ax.plot(X[my_members, 0], X[my_members, 1], 'w', markerfacecolor=col, marker='.',ms=10)\n\n    # Plots the centroids with specified color, but with a darker outline\n    ax.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,  markeredgecolor='k', markersize=6)\n\n# Title of the plot\nax.set_title('KMeans')\n\n# Remove x-axis ticks\nax.set_xticks(())\n\n# Remove y-axis ticks\nax.set_yticks(())\n\n# Show the plot\nplt.show()\n"]},{"cell_type":"markdown","id":"5dcf82cc-61a0-497a-a2dc-68f1f5a9357b","metadata":{},"outputs":[],"source":["### Exercise 1\n","Try to cluster the above dataset into a different number of clusters, say k=3. Note the difference in the pattern generated.\n"]},{"cell_type":"code","id":"f26dd262-694b-42b0-8b36-61d801974383","metadata":{},"outputs":[],"source":["# write your code here\n"]},{"cell_type":"markdown","id":"29771980-55a0-4351-b7f9-4b9dc4d5ed1d","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","k_means3 = KMeans(init=\"k-means++\", n_clusters=3, n_init=12)\n","k_means3.fit(X)\n","fig = plt.figure(figsize=(6, 4))\n","colors = plt.cm.tab10(np.linspace(0, 1, len(set(k_means3.labels_))))\n","ax = fig.add_subplot(1, 1, 1)\n","for k, col in zip(range(len(k_means3.cluster_centers_)), colors):\n","    my_members = (k_means3.labels_ == k)\n","    cluster_center = k_means3.cluster_centers_[k]\n","    ax.plot(X[my_members, 0], X[my_members, 1], 'w', markerfacecolor=col, marker='.',ms=10)\n","    ax.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,  markeredgecolor='k', markersize=6)\n","plt.show()\n","\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"baa56df3-234b-4242-b36b-f0dc202122e7","metadata":{},"outputs":[],"source":["### Exercise 2\n","Try the same with k=5.\n"]},{"cell_type":"code","id":"f6f304fd-ca19-46dc-ab57-bfc0de28c840","metadata":{},"outputs":[],"source":["# your code here"]},{"cell_type":"markdown","id":"93cbf76a-eb8e-498e-a8f6-d39b7dfcef69","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","k_means3 = KMeans(init=\"k-means++\", n_clusters=5, n_init=12)\n","k_means3.fit(X)\n","fig = plt.figure(figsize=(6, 4))\n","colors = plt.cm.tab10(np.linspace(0, 1, len(set(k_means3.labels_))))\n","ax = fig.add_subplot(1, 1, 1)\n","for k, col in zip(range(len(k_means3.cluster_centers_)), colors):\n","    my_members = (k_means3.labels_ == k)\n","    cluster_center = k_means3.cluster_centers_[k]\n","    ax.plot(X[my_members, 0], X[my_members, 1], 'w', markerfacecolor=col, marker='.',ms=10)\n","    ax.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,  markeredgecolor='k', markersize=6)\n","plt.show()\n","\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"7e453059-9eef-4f89-857c-6426f43d3ec6","metadata":{},"outputs":[],"source":["### Exercise 3\n","Comment on the within-cluster sum of squares, i.e. inertia, of the clusters created for k=3 and k=5. \n"]},{"cell_type":"markdown","id":"0d449055-cac4-4524-af6d-d2f3534ede2d","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","For k=3, the value of within-cluster sum of squares will be higher that that for k=4, since the points from different natural clusters are being grouped together, leading to underfitting of the k-means model. For k=5, the value of will be lesser than that for k=4, since the points are distributed into mode clusters than needed, leading to over-fitting of the k-means model.\n","</details>\n"]},{"cell_type":"markdown","id":"2deb3621-6565-4ebf-b7a9-a78b9eef3955","metadata":{},"outputs":[],"source":["<h1 id=\"customer_segmentation_K_means\">Customer Segmentation with k-means</h1>\n","\n","Imagine that you have a customer dataset, and you need to apply customer segmentation to this historical data.\n","Customer segmentation is the practice of partitioning a customer base into groups of individuals that have similar characteristics. It is a significant strategy as a business can target these specific groups of customers and effectively allocate marketing resources. For example, one group might contain customers who are high-profit and low-risk, or more likely to purchase products, or subscribe to a service. A business task is to retain those customers.\n"]},{"cell_type":"markdown","id":"6dcb6431-9ed8-4609-9715-dba87c723264","metadata":{},"outputs":[],"source":["### Load data from CSV file  \n","Let's use pandas to read the dataset from the cloud storage.\n"]},{"cell_type":"code","id":"184fd864-5234-40a4-814a-ce67a61d7c30","metadata":{},"outputs":[],"source":["cust_df = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%204/data/Cust_Segmentation.csv\")\ncust_df"]},{"cell_type":"markdown","id":"b73689d8-eabd-4369-88f2-d7e14ae2bb35","metadata":{},"outputs":[],"source":["<h2 id=\"pre_processing\">Pre-processing</h2\n"]},{"cell_type":"markdown","id":"ac3498bb-a2ca-4470-84e2-db84c38506f5","metadata":{},"outputs":[],"source":["As you can see, `Address` in this dataset is a categorical variable. The k-means algorithm doesn't work directly with categorical variables because the Euclidean distance function isn't meaningful for them. You could one-hot encode the categorical feature, but for illustration purposes let's run k-means clustering without it.\n"]},{"cell_type":"code","id":"3b59fe6c-d082-4793-86cd-b86bbc0ca5f3","metadata":{},"outputs":[],"source":["cust_df = cust_df.drop('Address', axis=1)"]},{"cell_type":"code","id":"40f40f79-8dc0-49ab-86aa-064b4163f270","metadata":{},"outputs":[],"source":["# Drop NaNs from the dataframe\ncust_df = cust_df.dropna()\ncust_df.info()"]},{"cell_type":"markdown","id":"c5641baf-1116-46a1-85cc-7e46419acbba","metadata":{},"outputs":[],"source":["After dropping NaNs we still have 700 rows out of the original 849. Let's proceed with this smaller data set.\n"]},{"cell_type":"markdown","id":"006aac3a-b9b5-4e39-88e9-9b1ca176be0c","metadata":{},"outputs":[],"source":["#### Normalizing over the standard deviation\n","Now let's normalize the dataset. But why do we need normalization in the first place? Normalization is a statistical method that helps mathematical-based algorithms to interpret features with different magnitudes and distributions equally, tranforming the features so they have zero mean and standard deviation of one. We use `StandardScaler()` to normalize, or standardize our dataset.\n"]},{"cell_type":"code","id":"c8a81bc6-b04d-46e2-97f1-2f82a888d234","metadata":{},"outputs":[],"source":["X = cust_df.values[:,1:] # leaves out `Customer ID`\nClus_dataSet = StandardScaler().fit_transform(X)"]},{"cell_type":"markdown","id":"4716f998-7c6c-4399-b3db-44ca13cb47a0","metadata":{},"outputs":[],"source":["<h2 id=\"modeling\">Modeling</h2>\n"]},{"cell_type":"markdown","id":"af78cc8e-908d-49d8-b8d9-e78b29f964a2","metadata":{},"outputs":[],"source":["Let's apply k-means to the data set. \n"]},{"cell_type":"markdown","id":"72759422-8483-4ce1-b0b3-17306280206f","metadata":{},"outputs":[],"source":["### Exercise 4\n","Write the code to cluster the data with k=3. Extract the cluster labels for this clustering process.\n"]},{"cell_type":"code","id":"9a3b0460-45a1-4873-9249-ec8d4424ba09","metadata":{},"outputs":[],"source":["# your code here.\nclusterNum = 3\nk_means = KMeans(...)\nk_means.fit(X)\nlabels = k_means.labels_"]},{"cell_type":"markdown","id":"46c81d53-d056-4d4b-83f4-ad97032a544d","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","clusterNum = 3\n","k_means = KMeans(init = \"k-means++\", n_clusters = clusterNum, n_init = 12)\n","k_means.fit(X)\n","labels = k_means.labels_\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"72a1d46a-20c1-4e65-ab90-99aa82b3bb88","metadata":{},"outputs":[],"source":["<h2 id=\"insights\">Insights</h2>\n","\n","We assign the k-means cluster labels to each row in the dataframe.\n"]},{"cell_type":"code","id":"de2b83fb-8c07-4dbf-a15b-d77e5b483759","metadata":{},"outputs":[],"source":["cust_df[\"Clus_km\"] = labels"]},{"cell_type":"markdown","id":"2e39bf19-b07b-414a-93d4-a1111559b720","metadata":{},"outputs":[],"source":["We can easily check the centroid values by averaging the features in each cluster. These values indicate the central point of the cluster from the vantage point of the field in question.\n"]},{"cell_type":"code","id":"2b51181b-de13-4bdf-98cc-5c82470af0ca","metadata":{},"outputs":[],"source":["cust_df.groupby('Clus_km').mean()"]},{"cell_type":"markdown","id":"cc7ee7c2-d8f3-42ab-affb-4305a4c1a5a7","metadata":{},"outputs":[],"source":["Now, let's look at the distribution of customers based on their education, age and income. We can choose to visualise this as a 2D scatter plot with Age on the x-axis, Income on the y-axis and the marker size representing education. The scatter points will be assigned different colors based on different class labels.\n"]},{"cell_type":"code","id":"5052181a-1a0d-4b39-88ac-c71b3ee9fce6","metadata":{},"outputs":[],"source":["area = np.pi * ( X[:, 1])**2  \nplt.scatter(X[:, 0], X[:, 3], s=area, c=labels.astype(float), cmap='tab10', ec='k',alpha=0.5)\nplt.xlabel('Age', fontsize=18)\nplt.ylabel('Income', fontsize=16)\nplt.show()"]},{"cell_type":"markdown","id":"38f5fca1-3047-4663-80ab-c67e9739e1d6","metadata":{},"outputs":[],"source":["We can also see this distribution in 3 dimensions for better understanding. Here, the education parameter will represent the third axis instead of the marker size. \n"]},{"cell_type":"code","id":"5a4cebf1-834b-4aaa-92c6-4781aeea9033","metadata":{},"outputs":[],"source":["# Create interactive 3D scatter plot\nfig = px.scatter_3d(X, x=1, y=0, z=3, opacity=0.7, color=labels.astype(float))\n\nfig.update_traces(marker=dict(size=5, line=dict(width=.25)), showlegend=False)\nfig.update_layout(coloraxis_showscale=False, width=1000, height=800, scene=dict(\n        xaxis=dict(title='Education'),\n        yaxis=dict(title='Age'),\n        zaxis=dict(title='Income')\n    ))  # Remove color bar, resize plot\n\nfig.show()\n"]},{"cell_type":"markdown","id":"f5d2b169-63c4-4f1d-96a2-1d0d56a8db5c","metadata":{},"outputs":[],"source":["k-means will partition your customers into mutually exclusive groups, for example, into 3 clusters. The customers in each cluster are similar to each other demographically.\n","\n","### Exercise 5\n","Create a profile for each group, considering the common characteristics of each cluster based on your observations. \n"]},{"cell_type":"code","id":"ccf3b776-3573-4bec-b661-a3301fc0ff90","metadata":{},"outputs":[],"source":["cust_df_sub = cust_df[['Age', 'Edu','Income','Clus_km']].copy() \nsns.pairplot(cust_df_sub, hue='Clus_km', palette='viridis', diag_kind='kde') \nplt.suptitle('Pairwise Scatter Plot with K-means Clusters', y=1.02)\nplt.show()"]},{"cell_type":"markdown","id":"94d40b16-a5f3-4581-a2a0-97fd9f5ff6c0","metadata":{},"outputs":[],"source":["#Enter your Obserrvation\n"]},{"cell_type":"markdown","id":"09dcbe56-a760-4905-bc50-776e2475851d","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","The 3 clusters can be:\n","\n","- LATE CAREER, AFFLUENT, AND EDUCATED\n","- MID CAREER AND MIDDLE INCOME\n","- EARLY CAREER AND LOW INCOME\n","\n","</details>\n"]},{"cell_type":"markdown","id":"c9f95787-6fe0-4bdf-9287-70f5aa7f6141","metadata":{},"outputs":[],"source":["### Congratulations! You're ready to move on to your next lesson!\n","\n","## Author\n","<a href=\"https://www.linkedin.com/in/abhishek-gagneja-23051987/\" target=\"_blank\">Abhishek Gagneja</a>\n","\n","### Other Contributors\n","<a href=\"https://www.linkedin.com/in/jpgrossman/\" target=\"_blank\">Jeff Grossman</a>  \n"," \n","<h3 align=\"center\"> © IBM Corporation. All rights reserved. <h3/>\n","\n"," \n","<!--\n","## Change Log\n"," \n"," \n","|  Date (YYYY-MM-DD) |  Version       | Changed By     | Change Description                  |\n","|---|---|---|---|\n","| 2024-10-31         | 3.0            | Jeff Grossman  | Rewrite                             |\n","| 2020-11-03         | 2.1            | Lakshmi        | Made changes in URL                 |\n","| 2020-11-03         | 2.1            | Lakshmi        | Made changes in URL                 |\n","| 2020-08-27         | 2.0            | Lavanya        | Moved lab to course repo in GitLab  |\n","|   |   |   |   |\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"prev_pub_hash":"0c77aed6100bfb9e958b348f7c9633a80588736c1ceab7ce31580eb2f4638ee9"},"nbformat":4,"nbformat_minor":4}